<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title><![CDATA[Mac系统下使用Hexo在Github上搭建博客]]></title>
    <url>%2F2019%2F02%2F23%2FMac%E7%B3%BB%E7%BB%9F%E4%B8%8B%E4%BD%BF%E7%94%A8Hexo%E5%9C%A8Github%E4%B8%8A%E6%90%AD%E5%BB%BA%E5%8D%9A%E5%AE%A2%2F</url>
    <content type="text"><![CDATA[一、安装环节二、Hexo部署三、github仓库四、git代码]]></content>
      <categories>
        <category>技术</category>
      </categories>
      <tags>
        <tag>博客</tag>
        <tag>技术</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[论文-类一致性]]></title>
    <url>%2F2019%2F02%2F23%2F%E8%AE%BA%E6%96%87-%E7%B1%BB%E4%B8%80%E8%87%B4%E6%80%A7DSC%2F</url>
    <content type="text"><![CDATA[文章贡献： • We propose class consistency as criteria for choosing good views to a class structure in n–D. Class consistency characterizes the extent to which the class neighborhood structure in n–D is preserved in a 2–D scatterplot, and thus avoids to label poor views as good views. • Since human attention is limited to inspect a small number of scatterplots, class consistency used as measure of goodness facilitates an interactive exploration of a class structure; otherwise a human analyst will be drown in the vast set of 2–D scatterplots. • We introduce and evaluate two methods for calculating class consistency, a distance based and a distribution based technique. Distribution based class consistency is more general, but more expensive to compute. • We evaluate class consistency over a range of data sets with different dimensionality. We show that the class consistency measures perform well in practice. First user experiments show that people rank consistent views better than inconsistent views. •在n-D中，我们建议将类一致性作为为类结构选择良好视图的标准。类一致性描述了在二维散点图中保留n-D中的类邻域结构的程度，从而避免了将不好的视图标记为好的视图。 •由于人类的注意力被限制在检查少量的散点图上，因此类的一致性被用来衡量好坏，从而促进了对类结构的交互式探索;否则，人类分析师就会被淹没在大量的二维散点图中。 •我们介绍并评估了两种计算类一致性的方法:基于距离的方法和基于分布的方法。基于分布的类一致性更通用，但计算起来更昂贵。 •我们在不同维度的数据集范围内评估类的一致性。结果表明，该方法在实际应用中效果良好。第一个用户实验表明，人们对一致的视图的排名要高于不一致的视图。 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990#!/usr/bin/env python# coding=utf-8from numpy import *import numpy as npimport csvfrom sklearn.datasets import load_irisimport matplotlib.pyplot as pltimport matplotlib as mplfrom sklearn.discriminant_analysis import LinearDiscriminantAnalysis# 加载数据def loadCSV(filename): dataSet = [] with open(filename, 'r') as file: csvReader = csv.reader(file) for line in csvReader: numbers = map(float, line) dataSet.append(numbers) dataSet = np.array(dataSet) X = dataSet[:, 1:len(dataSet)] y = dataSet[:, 0] y = map(int, y) return X, ydef LDA_dimensionality(X, y, k): ''' X为数据集，y为label，k为目标维数 ''' label_ = list(set(y)) X_classify = &#123;&#125; for label in label_: X1 = np.array([X[i] for i in range(len(X)) if y[i] == label]) X_classify[label] = X1 mju = np.mean(X, axis=0) mju_classify = &#123;&#125; for label in label_: mju1 = np.mean(X_classify[label], axis=0) mju_classify[label] = mju1 #St = np.dot((X - mju).T, X - mju) Sw = np.zeros((len(mju), len(mju))) # 计算类内散度矩阵 for i in label_: Sw += np.dot((X_classify[i] - mju_classify[i]).T, X_classify[i] - mju_classify[i]) # Sb=St-Sw Sb = np.zeros((len(mju), len(mju))) # 计算类内散度矩阵 for i in label_: Sb += len(X_classify[i]) * np.dot((mju_classify[i] - mju).reshape( (len(mju), 1)), (mju_classify[i] - mju).reshape((1, len(mju)))) eig_vals, eig_vecs = np.linalg.eig( np.linalg.inv(Sw).dot(Sb)) # 计算Sw-1*Sb的特征值和特征矩阵 sorted_indices = np.argsort(eig_vals) topk_eig_vecs = eig_vecs[:, sorted_indices[:-k - 1:-1]] # 提取前k个特征向量 print "eig_vecs" print eig_vecs print eig_vals print topk_eig_vecs return topk_eig_vecsif '__main__' == __name__: iris = load_iris() X = iris.data y = iris.target X, y = loadCSV('./data/ms_interleaved_40_80_3d_0.data') W = LDA_dimensionality(X, y, 2) X_new = np.dot((X), W) plt.figure(1) plt.scatter(X_new[:, 0], X_new[:, 1], marker='o', c=y) # 与sklearn中的LDA函数对比 lda = LinearDiscriminantAnalysis(n_components=2) lda.fit(X, y) X_new = lda.transform(X) # print(X_new) plt.figure(2) plt.scatter(X_new[:, 0], X_new[:, 1], marker='o', c=y) plt.show()]]></content>
      <categories>
        <category>论文</category>
      </categories>
      <tags>
        <tag>可视分析</tag>
        <tag>高维数据</tag>
      </tags>
  </entry>
</search>
